{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8115661,"sourceType":"datasetVersion","datasetId":4794729}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom collections import defaultdict\nfrom sklearn.metrics.pairwise import cosine_similarity","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-07-01T19:39:49.285704Z","iopub.execute_input":"2024-07-01T19:39:49.286179Z","iopub.status.idle":"2024-07-01T19:39:49.292301Z","shell.execute_reply.started":"2024-07-01T19:39:49.286144Z","shell.execute_reply":"2024-07-01T19:39:49.290896Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def load_and_preprocess_data(filepath):\n    data = pd.read_csv(filepath)\n    data = data.drop(columns=['ID'])  # Dropping the 'ID' column\n    # Normalize the data\n    data = data.dropna()\n    for col in data.columns:\n        data[col] = (data[col] - data[col].min()) / (data[col].max() - data[col].min())\n    return data","metadata":{"execution":{"iopub.status.busy":"2024-07-01T19:39:49.294529Z","iopub.execute_input":"2024-07-01T19:39:49.294870Z","iopub.status.idle":"2024-07-01T19:39:49.304954Z","shell.execute_reply.started":"2024-07-01T19:39:49.294832Z","shell.execute_reply":"2024-07-01T19:39:49.303715Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"filepath = \"/kaggle/input/xxxxxxxxxxxxxxxxxxx/custprofile.csv\"\ndf = load_and_preprocess_data(filepath)\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2024-07-01T19:39:49.306421Z","iopub.execute_input":"2024-07-01T19:39:49.306868Z","iopub.status.idle":"2024-07-01T19:39:49.388488Z","shell.execute_reply.started":"2024-07-01T19:39:49.306823Z","shell.execute_reply":"2024-07-01T19:39:49.387327Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nIndex: 2216 entries, 0 to 2239\nData columns (total 15 columns):\n #   Column               Non-Null Count  Dtype  \n---  ------               --------------  -----  \n 0   Income               2216 non-null   float64\n 1   Kidhome              2216 non-null   float64\n 2   Teenhome             2216 non-null   float64\n 3   Recency              2216 non-null   float64\n 4   MntWines             2216 non-null   float64\n 5   MntFruits            2216 non-null   float64\n 6   MntMeatProducts      2216 non-null   float64\n 7   MntFishProducts      2216 non-null   float64\n 8   MntSweetProducts     2216 non-null   float64\n 9   MntGoldProds         2216 non-null   float64\n 10  NumDealsPurchases    2216 non-null   float64\n 11  NumWebPurchases      2216 non-null   float64\n 12  NumCatalogPurchases  2216 non-null   float64\n 13  NumStorePurchases    2216 non-null   float64\n 14  NumWebVisitsMonth    2216 non-null   float64\ndtypes: float64(15)\nmemory usage: 277.0 KB\n","output_type":"stream"}]},{"cell_type":"code","source":"def k_means(data, k, iterations):\n    if isinstance(data, pd.DataFrame):\n        data = data.to_numpy()\n    np.random.seed(42)  # Ensure reproducibility\n    \n    # Initialize centroids randomly\n    initial_indices = np.random.choice(data.shape[0], size=k, replace=False)\n    centroids = data[initial_indices]\n\n    for _ in range(iterations):\n        clusters = defaultdict(list)\n\n        # Assign points to the nearest centroid\n        for index, point in enumerate(data):\n            distances = [1 - cosine_similarity([point], [centroid])[0][0] for centroid in centroids]\n            cluster_index = np.argmin(distances)\n            clusters[cluster_index].append(index)\n\n        # Update centroids\n        new_centroids = []\n        for cluster_indices in clusters.values():\n            cluster_data = data[cluster_indices]\n            new_centroids.append(np.mean(cluster_data, axis=0))\n        centroids = np.array(new_centroids)\n\n    return clusters\n","metadata":{"execution":{"iopub.status.busy":"2024-07-01T19:39:49.391100Z","iopub.execute_input":"2024-07-01T19:39:49.391468Z","iopub.status.idle":"2024-07-01T19:39:49.401853Z","shell.execute_reply.started":"2024-07-01T19:39:49.391422Z","shell.execute_reply":"2024-07-01T19:39:49.400649Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def silhouette_score(data, clusters):\n    data_np = np.array(data)\n    a_values, b_values = [], []\n    \n    cosine_distances = 1 - cosine_similarity(data_np)\n    \n    for index, point in enumerate(data_np):\n        for cluster_index, indices in clusters.items():\n            if index in indices:\n                current_cluster_indices = indices\n                break\n        \n        a = np.mean([cosine_distances[index, i] for i in current_cluster_indices if i != index]) if len(current_cluster_indices) > 1 else 0\n        b = np.inf\n        for other_cluster_index, other_indices in clusters.items():\n            if other_cluster_index != cluster_index:\n                dist = np.mean([cosine_distances[index, i] for i in other_indices])\n                b = min(b, dist)\n                \n        a_values.append(a)\n        b_values.append(b)\n    \n    s_scores = [(b - a) / max(a, b) if max(a, b) > 0 else 0 for a, b in zip(a_values, b_values)]\n    return np.mean(s_scores)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T19:39:49.403259Z","iopub.execute_input":"2024-07-01T19:39:49.403632Z","iopub.status.idle":"2024-07-01T19:39:49.417289Z","shell.execute_reply.started":"2024-07-01T19:39:49.403603Z","shell.execute_reply":"2024-07-01T19:39:49.415977Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# def k_means(data, k, iterations):\n#     # Ensure input is a NumPy array\n#     data_np = np.array(data) if not isinstance(data, np.ndarray) else data\n    \n#     # Randomly initialize centroids\n#     np.random.seed(42)\n#     initial_indices = np.random.choice(len(data_np), size=k, replace=False)\n#     centroids = data_np[initial_indices]\n    \n#     for _ in range(iterations):\n#         clusters = defaultdict(list)\n\n#         # Assign data points to the nearest cluster\n#         for index, point in enumerate(data_np):\n#             # Use distance as 1 - cosine similarity for clustering\n#             distances = [1 - cosine_similarity([point], [centroid])[0][0] for centroid in centroids]\n\n#             cluster_index = np.argmin(distances)\n#             clusters[cluster_index].append(index)\n        \n#         # Update centroids\n#         new_centroids = []\n#         for cluster_indices in clusters.values():\n#             if cluster_indices:  # Ensure the cluster is not empty\n#                 cluster_data = data_np[cluster_indices]\n#                 new_centroids.append(np.mean(cluster_data, axis=0))\n#             else:  # Handle the rare case of an empty cluster\n#                 new_centroids.append(centroids[len(new_centroids)])  # Reuse the old centroid\n#         centroids = new_centroids\n    \n#     # Save clustering information\n    \n#     return clusters\n\n# def silhouette_score(data, clusters):\n#     data_np = np.array(data) if not isinstance(data, np.ndarray) else data\n#     a_values, b_values = [], []\n    \n#     # Precompute all pairwise cosine distances for efficiency\n#     cosine_distances = 1 - cosine_similarity(data_np)\n    \n#     for index, point in enumerate(data_np):\n#         # Identify the cluster of the current point\n#         for cluster_index, indices in clusters.items():\n#             if index in indices:\n#                 current_cluster_indices = indices\n#                 break\n        \n#         # Compute 'a' value\n#         if len(current_cluster_indices) > 1:\n#             a = np.mean([cosine_distances[index, i] for i in current_cluster_indices if i != index])\n#         else:\n#             a = 0\n        \n#         # Compute 'b' value: smallest mean distance to all points in any other cluster\n#         b = np.inf\n#         for other_cluster_index, other_indices in clusters.items():\n#             if other_cluster_index != cluster_index:\n#                 dist = np.mean([cosine_distances[index, i] for i in other_indices])\n#                 b = min(b, dist)\n        \n#         a_values.append(a)\n#         b_values.append(b)\n    \n#     # Calculate silhouette scores\n#     s_scores = [(b - a) / max(a, b) if max(a, b) > 0 else 0 for a, b in zip(a_values, b_values)]\n    \n#     # Return the mean silhouette score\n#     return np.mean(s_scores)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T19:39:49.419076Z","iopub.execute_input":"2024-07-01T19:39:49.419547Z","iopub.status.idle":"2024-07-01T19:39:49.430618Z","shell.execute_reply.started":"2024-07-01T19:39:49.419513Z","shell.execute_reply":"2024-07-01T19:39:49.429346Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"best_k = 3\nbest_score = -1\nbest_clusters = defaultdict(list)\nfor k in range(3, 7):  # Exploring k values from 3 to 6\n    clusters = k_means(df, k, 20)\n#     print(clusters)\n    score = silhouette_score(df, clusters)\n    print(f\"Silhouette Score for k={k}: {score}\")\n    if score > best_score:\n        best_k, best_score = k, score\n        best_clusters = clusters\nprint(f\"Best k={best_k}\")\nprint(f\"Best Score for k={best_k}: {best_score}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-01T19:39:49.432385Z","iopub.execute_input":"2024-07-01T19:39:49.432856Z","iopub.status.idle":"2024-07-01T19:43:48.368586Z","shell.execute_reply.started":"2024-07-01T19:39:49.432822Z","shell.execute_reply":"2024-07-01T19:43:48.367378Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Silhouette Score for k=3: 0.40240389264387794\nSilhouette Score for k=4: 0.3626070522240497\nSilhouette Score for k=5: 0.36889402279492517\nSilhouette Score for k=6: 0.3344620014988835\nBest k=3\nBest Score for k=3: 0.40240389264387794\n","output_type":"stream"}]},{"cell_type":"code","source":"\nclass CustomTopDownClustering:\n    def __init__(self, min_clusters=3):\n        # Initialize the minimum number of clusters\n        self.min_clusters = min_clusters\n        # Store the indices of data points in each cluster\n        self.cluster_indices = []\n\n    def fit(self, X):\n        # Initialize with one cluster containing all data points\n        self.cluster_indices = [list(range(len(X)))]\n\n        # Continue splitting until reaching the desired number of clusters\n        while len(self.cluster_indices) < self.min_clusters:\n            # Find the cluster with the highest average cosine distance\n            max_avg_cos_distance = -1\n            split_cluster_index = None\n\n            # Iterate over existing clusters to find the one to split\n            for i, cluster_index in enumerate(self.cluster_indices):\n                # Calculate the average cosine distance within the cluster\n                avg_cos_distance = self._average_cosine_distance(X, cluster_index)\n                # Update if the current cluster has higher average cosine distance\n                if avg_cos_distance > max_avg_cos_distance:\n                    max_avg_cos_distance = avg_cos_distance\n                    split_cluster_index = i\n\n            # Split the cluster along the dimension with the highest average cosine distance\n            split_cluster_indices = self.cluster_indices[split_cluster_index]\n\n            # Perform the split\n            left_indices, right_indices = self._split_data(X, split_cluster_indices)\n\n            # Update the cluster list with the two split clusters\n            self.cluster_indices.pop(split_cluster_index)\n            self.cluster_indices.append(left_indices)\n            self.cluster_indices.append(right_indices)\n            \n        return self.cluster_indices\n\n    def cosine_distance(self, a, b):\n        # Compute the cosine distance between two vectors\n        dot_product = np.dot(a, b)\n        norm_a = np.linalg.norm(a)\n        norm_b = np.linalg.norm(b)\n        return 1 - (dot_product / (norm_a * norm_b))\n\n    def _average_cosine_distance(self, X, cluster_indices):\n        # Calculate the average cosine distance within a cluster\n        distances = []\n        for i in range(len(cluster_indices)):\n            for j in range(i + 1, len(cluster_indices)):\n                distances.append(self.cosine_distance(X[cluster_indices[i]], X[cluster_indices[j]]))\n        return np.mean(distances)\n    \n    def _split_data(self, X, cluster_indices):\n        # Initialize cluster centroids randomly\n        while True:\n            centroid1_idx, centroid2_idx = np.random.choice(len(cluster_indices), size=2, replace=False)\n            if centroid1_idx != centroid2_idx:\n                break\n        centroid1 = X[cluster_indices[centroid1_idx]]\n        centroid2 = X[cluster_indices[centroid2_idx]]\n\n        # Perform K-means clustering to split the data\n        for _ in range(100):  # Number of iterations for K-means\n            cluster1_indices = []\n            cluster2_indices = []\n            for idx in cluster_indices:\n                # Assign each point to the nearest centroid\n                dist_to_centroid1 = self.cosine_distance(X[idx], centroid1)\n                dist_to_centroid2 = self.cosine_distance(X[idx], centroid2)\n                if dist_to_centroid1 < dist_to_centroid2:\n                    cluster1_indices.append(idx)\n                else:\n                    cluster2_indices.append(idx)\n\n            # Update centroids based on the mean of points in each cluster\n            centroid1 = np.mean(X[cluster1_indices], axis=0)\n            centroid2 = np.mean(X[cluster2_indices], axis=0)\n\n        return cluster1_indices, cluster2_indices\n","metadata":{"execution":{"iopub.status.busy":"2024-07-01T19:43:48.370079Z","iopub.execute_input":"2024-07-01T19:43:48.370418Z","iopub.status.idle":"2024-07-01T19:43:48.389637Z","shell.execute_reply.started":"2024-07-01T19:43:48.370390Z","shell.execute_reply":"2024-07-01T19:43:48.388177Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"model= CustomTopDownClustering(min_clusters=best_k)\nX=df.values\nheir_cluster_points = model.fit(X)\n\nfor i in range (len(heir_cluster_points)):\n    heir_cluster_points[i]=np.array(heir_cluster_points[i])\n    \n    \nfor i in range(len(heir_cluster_points)):\n    heir_cluster_points[i] = np.sort(heir_cluster_points[i])\n\n# Sort the list of arrays based on the first element of each array\nheir_cluster_points.sort(key=lambda y: y[0])\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-01T19:43:48.393419Z","iopub.execute_input":"2024-07-01T19:43:48.394392Z","iopub.status.idle":"2024-07-01T19:45:02.169650Z","shell.execute_reply.started":"2024-07-01T19:43:48.394345Z","shell.execute_reply":"2024-07-01T19:45:02.168547Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"best_cluster_points=[]\nfor i in range(len(best_clusters)):\n    best_cluster_points.append(best_clusters[i])\nfor i in range (len(best_cluster_points)):\n    best_cluster_points[i]=np.array(best_cluster_points[i])\n#     print(best_cluster_points[i])\n# print(best_cluster_points)    \n    \nfor i in range(len(best_cluster_points)):\n    best_cluster_points[i] = np.sort(best_cluster_points[i])\n\n# Sort the list of arrays based on the first element of each array\nbest_cluster_points.sort(key=lambda y: y[0])\nwith open(\"kmeans.txt\", \"w\") as f:\n    f.write(\"Number of clusters = {best_score}\\n\")\n    for row in best_cluster_points:\n        f.write(', '.join(map(str, row)))\n        f.write('\\n')\n        \nwith open(\"divisive.txt\", \"w\") as f:\n    f.write(\"Number of clusters = {}\\n\".format(best_k))\n    for row in heir_cluster_points:\n        f.write(', '.join(map(str, row)))\n        f.write('\\n')","metadata":{"execution":{"iopub.status.busy":"2024-07-01T19:45:02.171072Z","iopub.execute_input":"2024-07-01T19:45:02.171456Z","iopub.status.idle":"2024-07-01T19:45:02.185505Z","shell.execute_reply.started":"2024-07-01T19:45:02.171406Z","shell.execute_reply":"2024-07-01T19:45:02.184322Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef jaccard_similarity(arrA, arrB):\n    # Calculate the Jaccard similarity between two arrays\n    intersection = len(np.intersect1d(arrA, arrB))\n    union = len(np.union1d(arrA, arrB))\n    return intersection / union if union != 0 else 0\n\ndef mapping_and_similarity(list1, list2):\n    # Function to compute similarity scores and mappings between two lists of arrays\n    similarity_scores = []\n    \n    # Iterate over each array in list1\n    for arrA in list1:\n        max_similarity = 0\n        best_mapping = None\n        \n        # Iterate over each array in list2\n        for arrB in list2:\n            # Check if arrB has been used in previous mappings\n            if any(np.array_equal(arrB, mapping[1]) for mapping in similarity_scores):\n                continue\n            \n            # Compute Jaccard similarity for current mapping\n            similarity = jaccard_similarity(arrA, arrB)\n            \n            # Update max similarity and corresponding mapping\n            if similarity > max_similarity:\n                max_similarity = similarity\n                best_mapping = (arrA, arrB)\n        \n        # Store the max similarity and corresponding mapping for the current array in list1\n        similarity_scores.append((max_similarity, best_mapping))\n    \n    return similarity_scores\n\n# Example usage\n# Compute similarity scores between heir_cluster_points and best_cluster_points\nsimilarity_scores = mapping_and_similarity(heir_cluster_points, best_cluster_points)\n\n# Print the results\nfor i, (similarity, mapping) in enumerate(similarity_scores):\n    print(f\"Jaccard Similarity for pair {i+1}: {similarity}\")\n    # Additional details can be printed if needed\n    # print(\"For cluster of divisive\\n\",mapping[0])\n    # print(\"Mapping to kmeans cluster\\n\", mapping[1])\n    print()\n\n# Compute average Jaccard Similarity\naverage_similarity=0.0\nfor (similarity,mapping) in (similarity_scores):\n    average_similarity+=similarity\n\naverage_similarity/=best_k\nprint(f\"Average Jaccard Similarity: {average_similarity}\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-01T19:45:02.187290Z","iopub.execute_input":"2024-07-01T19:45:02.187751Z","iopub.status.idle":"2024-07-01T19:45:02.203967Z","shell.execute_reply.started":"2024-07-01T19:45:02.187712Z","shell.execute_reply":"2024-07-01T19:45:02.202708Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Jaccard Similarity for pair 1: 0.9161490683229814\n\nJaccard Similarity for pair 2: 0.8137869292748433\n\nJaccard Similarity for pair 3: 0.6368715083798883\n\nAverage Jaccard Similarity: 0.7889358353259043\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}